{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "# length of time series\n",
    "start, end = \"1900-01-01\", \"2025-12-31\"\n",
    "\n",
    "# FRED ID\n",
    "fred_ids = {\n",
    "    \"CPI\": \"CPIAUCSL\",\n",
    "    \"PPI\": \"PPIACO\",\n",
    "    #\"Unemployment\": \"UNRATE\",\n",
    "    #\"IndustrialProduction\": \"INDPRO\",\n",
    "    #\"M2\": \"M2SL\",\n",
    "    #\"UST10Y\": \"GS10\",\n",
    "    #\"UST3M\": \"GS3M\",\n",
    "    #\"SP500\": \"SP500\",\n",
    "   \n",
    "}\n",
    "\n",
    "# obtain data\n",
    "df = pd.DataFrame()\n",
    "for name, fid in fred_ids.items():\n",
    "    try:\n",
    "        df[name] = web.DataReader(fid, \"fred\", start, end)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {name} ({fid}): {e}\")\n",
    "\n",
    "# transfer to monthly frequency\n",
    "df_monthly = df.resample(\"M\").last()\n",
    "\n",
    "# first order difference\n",
    "returns = pd.DataFrame()\n",
    "for col in df_monthly.columns:\n",
    "    series = df_monthly[col].dropna()\n",
    "    if (series <= 0).any():\n",
    "        r = series.diff().dropna()\n",
    "    else:\n",
    "        r = np.log(series).diff().dropna()\n",
    "    returns[col] = r\n",
    "\n",
    "# skewness and excess kurtosis\n",
    "stats = pd.DataFrame({\n",
    "    \"Skewness\": returns.skew(),\n",
    "    \"Kurtosis\": returns.kurtosis()\n",
    "})\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, Optional\n",
    "\n",
    "def load_uk_rpi_mixed_csv(\n",
    "    path: str,\n",
    "    value_col: int = 1,\n",
    "    date_col: int = 0,\n",
    "    keep_annual: Literal[\"drop\", \"jan\", \"dec\"] = \"drop\",\n",
    "    uppercase_month=True,\n",
    ") -> pd.Series:\n",
    "   \n",
    "    raw = pd.read_csv(path, header=None, usecols=[date_col, value_col], dtype=str)\n",
    "    raw = raw.rename(columns={date_col: \"period\", value_col: \"value\"})\n",
    "\n",
    "    raw[\"period\"] = raw[\"period\"].astype(str).str.strip()\n",
    "    raw[\"value\"] = raw[\"value\"].astype(str).str.strip()\n",
    "    raw = raw.replace({\"\": np.nan, \"nan\": np.nan}).dropna(subset=[\"period\", \"value\"])\n",
    "\n",
    "    pat_year = re.compile(r\"^\\s*(\\d{4})\\s*$\")\n",
    "    pat_month = re.compile(r\"^\\s*(\\d{4})\\s+([A-Za-z]{3})\\s*$\")\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, row in raw.iterrows():\n",
    "        p = row[\"period\"]\n",
    "        v = row[\"value\"]\n",
    "        try:\n",
    "            val = float(v)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        m_month = pat_month.match(p)\n",
    "        m_year = pat_year.match(p)\n",
    "\n",
    "        if m_month:\n",
    "            yyyy = int(m_month.group(1))\n",
    "            mon_abbr = m_month.group(2)\n",
    "            if uppercase_month:\n",
    "                mon_abbr = mon_abbr.upper()\n",
    "\n",
    "            m_map = dict(JAN=1, FEB=2, MAR=3, APR=4, MAY=5, JUN=6,\n",
    "                         JUL=7, AUG=8, SEP=9, OCT=10, NOV=11, DEC=12)\n",
    "            if mon_abbr not in m_map:\n",
    "                if mon_abbr.startswith(\"SE\"):\n",
    "                    mon_abbr = \"SEP\"\n",
    "                if mon_abbr not in m_map:\n",
    "                    continue\n",
    "            mm = m_map[mon_abbr]\n",
    "            dt = pd.Timestamp(year=yyyy, month=mm, day=1)  # 月初\n",
    "            records.append((dt, val, \"M\"))\n",
    "\n",
    "        elif m_year:\n",
    "            if keep_annual == \"drop\":\n",
    "                continue\n",
    "            yyyy = int(m_year.group(1))\n",
    "            mm = 1 if keep_annual == \"jan\" else 12\n",
    "            dt = pd.Timestamp(year=yyyy, month=mm, day=1)\n",
    "            records.append((dt, val, \"Y\"))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if not records:\n",
    "        raise ValueError(\"NO valid data.\")\n",
    "\n",
    "    df = pd.DataFrame(records, columns=[\"date\", \"value\", \"freq\"]).sort_values(\"date\")\n",
    "    s = df.set_index(\"date\")[\"value\"].astype(float)\n",
    "\n",
    "    s = s.asfreq(\"MS\") \n",
    "    s.name = \"UK_RPI\"\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPI, obtaind fron ONS\n",
    "uk_rpi = load_uk_rpi_mixed_csv(\"/Users/hanshuting/Desktop/RPI.csv\", keep_annual=\"drop\")\n",
    "\n",
    "def to_monthly_growth(series: pd.Series) -> pd.Series:\n",
    "    s = series.dropna()\n",
    "    if (s <= 0).any():\n",
    "        return s.diff().dropna()\n",
    "    return np.log(s).diff().dropna()\n",
    "\n",
    "rpi_growth = to_monthly_growth(uk_rpi)\n",
    "skew_excess = rpi_growth.skew()           \n",
    "kurt_excess = rpi_growth.kurtosis()       \n",
    "\n",
    "print({\"skew\": skew_excess, \"kurt_excess\": kurt_excess})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Tuple\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gammaln\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# ---------- helpers: link functions ----------\n",
    "def softplus(x):  # >0\n",
    "    return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# ---------- map raw -> constrained (omega>0, alpha>=0, beta>=0, alpha+beta<1) ----------\n",
    "def map_garch_params(theta_raw: np.ndarray) -> Tuple[float, float, float]:\n",
    "    # (omega, alpha, beta)\n",
    "    log_omega, s_raw, r_raw = theta_raw[:3]\n",
    "    omega = np.exp(log_omega) + EPS\n",
    "    A = softplus(s_raw)\n",
    "    B = softplus(r_raw)\n",
    "    total = 1.0 + A + B\n",
    "    alpha = A / total * (1.0 - 1e-3)\n",
    "    beta  = B / total * (1.0 - 1e-3)\n",
    "    return omega, alpha, beta\n",
    "\n",
    "def logpdf_gauss(z: np.ndarray) -> np.ndarray:\n",
    "    return -0.5 * (np.log(2.0 * np.pi) + z**2)\n",
    "\n",
    "def logpdf_t_unitvar(z: np.ndarray, nu: float) -> np.ndarray:\n",
    "    # z = sqrt((nu-2)/nu) * u, u ~ StudentT(nu)\n",
    "    # => log f_Z(z) = log f_U(u) + log(du/dz) = log t_pdf(u;nu) + 0.5*log(nu/(nu-2))\n",
    "    u = z * np.sqrt(nu / (nu - 2.0))\n",
    "    c = 0.5 * np.log(nu / (nu - 2.0))\n",
    "    # log c_nu = lgamma((nu+1)/2) - lgamma(nu/2) - 0.5*log(nu*pi)\n",
    "    log_c = gammaln(0.5 * (nu + 1.0)) - gammaln(0.5 * nu) - 0.5 * (np.log(nu) + np.log(np.pi))\n",
    "    return log_c - 0.5 * (nu + 1.0) * np.log1p((u**2) / nu) + c\n",
    "\n",
    "def logpdf_beta_unitvar(z: np.ndarray, a: float, b: float) -> np.ndarray:\n",
    "    # 若 Y ~ Beta(a,b) on (0,1), 则 Z = (Y - m)/s, m=a/(a+b), s=sqrt(ab/((a+b)^2 (a+b+1)))\n",
    "    m = a / (a + b)\n",
    "    s = np.sqrt(a * b / (((a + b) ** 2) * (a + b + 1.0)))\n",
    "    y = s * z + m\n",
    "    mask = (y > 0.0) & (y < 1.0)\n",
    "    out = np.full_like(z, fill_value=-1e12, dtype=float)\n",
    "    # log Beta(a,b) = lgamma(a)+lgamma(b)-lgamma(a+b)\n",
    "    log_B = gammaln(a) + gammaln(b) - gammaln(a + b)\n",
    "    # log f_Y(y) = (a-1)log y + (b-1)log(1-y) - log_B\n",
    "    yy = np.clip(y[mask], EPS, 1.0 - EPS)\n",
    "    log_fy = (a - 1.0) * np.log(yy) + (b - 1.0) * np.log1p(-yy) - log_B\n",
    "    # f_Z(z) = f_Y(y) * |dy/dz| = f_Y(y) * s  ⇒ log f_Z = log f_Y + log s\n",
    "    out[mask] = log_fy + np.log(s + EPS)\n",
    "    return out\n",
    "\n",
    "# ---------- GARCH(1,1) recursion ----------\n",
    "def garch_sigma2(resid: np.ndarray, omega: float, alpha: float, beta: float) -> np.ndarray:\n",
    "    n = resid.size\n",
    "    sigma2 = np.empty(n, dtype=float)\n",
    "    longrun = omega / max(1.0 - alpha - beta, 1e-6)\n",
    "    sigma2[0] = longrun\n",
    "    for t in range(1, n):\n",
    "        sigma2[t] = omega + alpha * (resid[t-1] ** 2) + beta * sigma2[t-1]\n",
    "        \n",
    "        if sigma2[t] < 1e-12:\n",
    "            sigma2[t] = 1e-12\n",
    "    return sigma2\n",
    "\n",
    "\n",
    "def nll_garch(theta_raw: np.ndarray, y: np.ndarray, dist: str) -> float:\n",
    "   \n",
    "    #   gaussian: [mu_raw, log_omega, s_raw, r_raw]\n",
    "    #   t       : [mu_raw, log_omega, s_raw, r_raw, nu_raw]\n",
    "    #   beta    : [mu_raw, log_omega, s_raw, r_raw, a_raw, b_raw]\n",
    "    mu = theta_raw[0]\n",
    "    omega, alpha, beta = map_garch_params(theta_raw[1:4])\n",
    "    e = y - mu\n",
    "    sigma2 = garch_sigma2(e, omega, alpha, beta)\n",
    "    z = e / np.sqrt(sigma2)\n",
    "\n",
    "    if dist == \"gaussian\":\n",
    "        logfz = logpdf_gauss(z)\n",
    "    elif dist == \"t\":\n",
    "        nu = 4.0 + softplus(theta_raw[4])      # v>4 \n",
    "        logfz = logpdf_t_unitvar(z, nu)\n",
    "    elif dist == \"beta\":\n",
    "        a = 2.0 + softplus(theta_raw[4])       # a,b >2\n",
    "        b = 2.0 + softplus(theta_raw[5])\n",
    "        logfz = logpdf_beta_unitvar(z, a, b)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dist\")\n",
    "\n",
    "    # p(e_t) = (1/sigma_t) * f_Z(z_t) ⇒ loglik = sum(log f_Z(z_t) - 0.5*log sigma2_t)\n",
    "    ll = np.sum(logfz - 0.5 * np.log(sigma2))\n",
    "    return float(-ll)\n",
    "\n",
    "def information_criteria(n: int, k: int, loglik: float):\n",
    "    aic  = 2*k - 2*loglik\n",
    "    bic  = np.log(n)*k - 2*loglik\n",
    "    if n > k + 1:\n",
    "        aicc = aic + (2*k*(k+1)) / (n - k - 1)\n",
    "    else:\n",
    "        aicc = np.nan\n",
    "    hqic = 2*k*np.log(np.log(max(n,3))) - 2*loglik\n",
    "    return {\"AIC\": aic, \"BIC\": bic, \"AICc\": aicc, \"HQIC\": hqic}\n",
    "\n",
    "@dataclass\n",
    "class FitResult:\n",
    "    dist: str\n",
    "    params: Dict[str, float]\n",
    "    loglik: float\n",
    "    aic: float\n",
    "    bic: float\n",
    "    success: bool\n",
    "    message: str\n",
    "    ic: Dict[str, float] = None\n",
    "\n",
    "def fit_garch_mle(y: np.ndarray, dist: str = \"gaussian\") -> FitResult:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y[np.isfinite(y)]\n",
    "    n = y.size\n",
    "\n",
    "    if dist == \"gaussian\":\n",
    "        x0 = np.array([np.mean(y), np.log(np.var(y)+1e-3), 0.1, 0.9])\n",
    "    elif dist == \"t\":\n",
    "        x0 = np.array([np.mean(y), np.log(np.var(y)+1e-3), 0.1, 0.9, np.log(np.e)])  # nu_raw≈1→nu≈4+1=5\n",
    "    elif dist == \"beta\":\n",
    "        x0 = np.array([np.mean(y), np.log(np.var(y)+1e-3), 0.1, 0.9, np.log(np.e), np.log(np.e)])\n",
    "    else:\n",
    "        raise ValueError(\"dist must be 'gaussian', 't', or 'beta'.\")\n",
    "\n",
    "    obj = lambda th: nll_garch(th, y, dist)\n",
    "    res = minimize(obj, x0, method=\"L-BFGS-B\")\n",
    "\n",
    "    mu = res.x[0]\n",
    "    omega, alpha, beta = map_garch_params(res.x[1:4])\n",
    "\n",
    "    params = {\"mu\": mu, \"omega\": omega, \"alpha\": alpha, \"beta\": beta}\n",
    "    k = 4\n",
    "    if dist == \"t\":\n",
    "        nu = 4.0 + softplus(res.x[4])\n",
    "        params[\"nu\"] = nu\n",
    "        k += 1\n",
    "    if dist == \"beta\":\n",
    "        a = 2.0 + softplus(res.x[4])\n",
    "        b = 2.0 + softplus(res.x[5])\n",
    "        params[\"a\"] = a\n",
    "        params[\"b\"] = b\n",
    "        k += 2\n",
    "\n",
    "    loglik = -obj(res.x)\n",
    "    aic = 2 * k - 2 * loglik\n",
    "    bic = np.log(n) * k - 2 * loglik\n",
    "    ic = information_criteria(n, k, loglik)\n",
    "    return FitResult(dist=dist, params=params, loglik=loglik,\n",
    "                 aic=aic, bic=bic, success=bool(res.success),\n",
    "                 message=res.message, ic=ic)\n",
    "\n",
    "def to_returns_monthly(x: pd.Series) -> pd.Series:\n",
    "    x = x.dropna()\n",
    "    if (x <= 0).any():\n",
    "        r = x.diff()\n",
    "    else:\n",
    "        r = np.log(x).diff()\n",
    "    return r.dropna()\n",
    "\n",
    "def compare_three(y: pd.Series) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for d in [\"gaussian\", \"t\", \"beta\"]:\n",
    "        fr = fit_garch_mle(y.values, dist=d)\n",
    "        row = {\"dist\": d, \"loglik\": fr.loglik, \"AIC\": fr.aic, \"BIC\": fr.bic, \"success\": fr.success}\n",
    "        row.update(fr.params)\n",
    "        out.append(row)\n",
    "    return pd.DataFrame(out).set_index(\"dist\").sort_values(\"BIC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = to_returns_monthly(df_monthly[\"CPI\"])     \n",
    "summary1 = compare_three(y1)\n",
    "print(summary1)\n",
    "fr_g1 = fit_garch_mle(y1.values, dist=\"gaussian\")\n",
    "fr_t1 = fit_garch_mle(y1.values, dist=\"t\")\n",
    "fr_b1 = fit_garch_mle(y1.values, dist=\"beta\")\n",
    "\n",
    "def ic_table(*fit_results):\n",
    "    rows = []\n",
    "    for fr in fit_results:\n",
    "        rows.append({\n",
    "            \"dist\": fr.dist.title(),\n",
    "            \"AIC\": fr.ic[\"AIC\"], \"BIC\": fr.ic[\"BIC\"],\n",
    "            \"AICc\": fr.ic[\"AICc\"], \"HQIC\": fr.ic[\"HQIC\"]\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"dist\").round(4)\n",
    "\n",
    "df_ic1 = ic_table(fr_g1, fr_t1, fr_b1)\n",
    "display(df_ic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = to_returns_monthly(df_monthly[\"PPI\"])     \n",
    "summary2 = compare_three(y2)\n",
    "print(summary2)\n",
    "fr_g2 = fit_garch_mle(y2.values, dist=\"gaussian\")\n",
    "fr_t2 = fit_garch_mle(y2.values, dist=\"t\")\n",
    "fr_b2 = fit_garch_mle(y2.values, dist=\"beta\")\n",
    "\n",
    "def ic_table(*fit_results):\n",
    "    rows = []\n",
    "    for fr in fit_results:\n",
    "        rows.append({\n",
    "            \"dist\": fr.dist.title(),\n",
    "            \"AIC\": fr.ic[\"AIC\"], \"BIC\": fr.ic[\"BIC\"],\n",
    "            \"AICc\": fr.ic[\"AICc\"], \"HQIC\": fr.ic[\"HQIC\"]\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"dist\").round(4)\n",
    "\n",
    "df_ic2 = ic_table(fr_g2, fr_t2, fr_b2)\n",
    "display(df_ic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t as tdist, beta as betadist\n",
    "\n",
    "def q_gauss(alpha): return float(norm.ppf(alpha))\n",
    "\n",
    "def q_t_unitvar(alpha, nu):\n",
    "    return float(np.sqrt((nu-2.0)/nu) * tdist.ppf(alpha, df=nu))\n",
    "\n",
    "def q_beta_std(alpha, a, b):\n",
    "    m = a/(a+b)\n",
    "    s = np.sqrt(a*b/(((a+b)**2)*(a+b+1.0)))\n",
    "    y = float(betadist.ppf(alpha, a, b))\n",
    "    return (y - m)/s\n",
    "\n",
    "def rvs_unitvar(dist: str, size: int, **kw):\n",
    "    if dist == \"gaussian\":\n",
    "        return np.random.randn(size)\n",
    "    if dist == \"t\":\n",
    "        nu = kw[\"nu\"]; return np.sqrt((nu-2.0)/nu) * tdist.rvs(df=nu, size=size)\n",
    "    if dist == \"beta\":\n",
    "        a, b = kw[\"a\"], kw[\"b\"]\n",
    "        y = betadist.rvs(a, b, size=size)\n",
    "        m = a/(a+b); s = np.sqrt(a*b/(((a+b)**2)*(a+b+1.0)))\n",
    "        return (y - m)/s\n",
    "    raise ValueError(\"unknown dist\")\n",
    "\n",
    "# --- σ²_{T+1|T: T+h|T} ---\n",
    "def forecast_sigma2_path(last_e2, last_sigma2, omega, alpha, beta, h):\n",
    "    out = np.empty(h, dtype=float)\n",
    "    s = omega + alpha*last_e2 + beta*last_sigma2\n",
    "    out[0] = max(s, 1e-12)\n",
    "    for i in range(1, h):\n",
    "        s = omega + (alpha + beta) * out[i-1]\n",
    "        out[i] = max(s, 1e-12)\n",
    "    return out\n",
    "\n",
    "# --- 1-step VaR ---\n",
    "def var_1step(mu, sigma2_1, dist, **kw):\n",
    "    s = np.sqrt(sigma2_1)\n",
    "    alpha = kw.get(\"alpha\", 0.01)\n",
    "    if dist == \"gaussian\":\n",
    "        q = q_gauss(alpha)\n",
    "    elif dist == \"t\":\n",
    "        q = q_t_unitvar(alpha, kw[\"nu\"])\n",
    "    elif dist == \"beta\":\n",
    "        q = q_beta_std(alpha, kw[\"a\"], kw[\"b\"])\n",
    "    else:\n",
    "        raise ValueError(\"unknown dist\")\n",
    "    return mu + q * s\n",
    "\n",
    "# --- h-step VaR（analytic/MC） ---\n",
    "def var_hstep(mu, sigma2_path, dist, alpha, method=\"analytic\", mc_paths=20000, **kw):\n",
    "    h = len(sigma2_path)\n",
    "    if method == \"analytic\":\n",
    "        s_tot = np.sqrt(np.sum(sigma2_path))\n",
    "        q = q_gauss(alpha)  \n",
    "        return h*mu + q*s_tot\n",
    "    elif method == \"mc\":\n",
    "        Z = np.empty((mc_paths, h))\n",
    "        if dist == \"gaussian\":\n",
    "            for s in range(h): Z[:, s] = rvs_unitvar(\"gaussian\", mc_paths)\n",
    "        elif dist == \"t\":\n",
    "            for s in range(h): Z[:, s] = rvs_unitvar(\"t\", mc_paths, nu=kw[\"nu\"])\n",
    "        else:\n",
    "            for s in range(h): Z[:, s] = rvs_unitvar(\"beta\", mc_paths, a=kw[\"a\"], b=kw[\"b\"])\n",
    "        R = (mu + Z * np.sqrt(sigma2_path)[None, :]).sum(axis=1)\n",
    "        return float(np.quantile(R, alpha))\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'analytic' or 'mc'\")\n",
    "\n",
    "# --- 1-step VaR（rolling）---\n",
    "def one_step_var_series(y, fr, alpha):\n",
    "    mu = fr.params[\"mu\"]; omega = fr.params[\"omega\"]\n",
    "    a = fr.params[\"alpha\"]; b = fr.params[\"beta\"]\n",
    "    y = np.asarray(y); e = y - mu\n",
    "    sigma2 = garch_sigma2(e, omega, a, b)\n",
    "    var = np.empty_like(y, dtype=float)\n",
    "    if fr.dist == \"gaussian\":\n",
    "        for t in range(y.size):\n",
    "            var[t] = var_1step(mu, sigma2[t], \"gaussian\", alpha=alpha)\n",
    "    elif fr.dist == \"t\":\n",
    "        nu = fr.params[\"nu\"]\n",
    "        for t in range(y.size):\n",
    "            var[t] = var_1step(mu, sigma2[t], \"t\", alpha=alpha, nu=nu)\n",
    "    else:\n",
    "        A, B = fr.params[\"a\"], fr.params[\"b\"]\n",
    "        for t in range(y.size):\n",
    "            var[t] = var_1step(mu, sigma2[t], \"beta\", alpha=alpha, a=A, b=B)\n",
    "    return var\n",
    "\n",
    "# --- h-step VaR（rolling）---\n",
    "def h_step_var_series(y, fr, alpha, h, method=\"analytic\", mc_paths=30000):\n",
    "    mu = fr.params[\"mu\"]; omega = fr.params[\"omega\"]\n",
    "    a = fr.params[\"alpha\"]; b = fr.params[\"beta\"]\n",
    "    y = np.asarray(y); e = y - mu\n",
    "    sigma2 = garch_sigma2(e, omega, a, b)\n",
    "    var = np.empty_like(y, dtype=float)\n",
    "    for t in range(y.size):\n",
    "        last_e2 = e[t-1]**2 if t-1 >= 0 else omega/(1.0 - a - b)\n",
    "        last_s2 = sigma2[t-1] if t-1 >= 0 else omega/(1.0 - a - b)\n",
    "        s_path = forecast_sigma2_path(last_e2, last_s2, omega, a, b, h)\n",
    "        if fr.dist == \"gaussian\":\n",
    "            var[t] = var_hstep(mu, s_path, \"gaussian\", alpha, method, mc_paths)\n",
    "        elif fr.dist == \"t\":\n",
    "            var[t] = var_hstep(mu, s_path, \"t\", alpha, method, mc_paths, nu=fr.params[\"nu\"])\n",
    "        else:\n",
    "            var[t] = var_hstep(mu, s_path, \"beta\", alpha, method, mc_paths,\n",
    "                               a=fr.params[\"a\"], b=fr.params[\"b\"])\n",
    "    return var\n",
    "\n",
    "# --- VaR backtesting（UC/IND/CC） ---\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def var_backtests(y, var, alpha):\n",
    "    y = np.asarray(y); var = np.asarray(var)\n",
    "    m = np.isfinite(y) & np.isfinite(var)\n",
    "    y = y[m]; var = var[m]; n = y.size\n",
    "    I = (y < var).astype(int)  \n",
    "\n",
    "    # UC\n",
    "    n1 = I.sum(); p_hat = n1 / n if n>0 else 0.0\n",
    "    def _logp(p): \n",
    "        p = np.clip(p, 1e-12, 1-1e-12)\n",
    "        return np.log(p)\n",
    "    def L_bin(k, N, p):\n",
    "        p = np.clip(p, 1e-12, 1-1e-12)\n",
    "        return k*_logp(p) + (N-k)*_logp(1-p)\n",
    "    LR_uc = -2*(L_bin(n1, n, alpha) - L_bin(n1, n, p_hat))\n",
    "    p_uc  = 1 - chi2.cdf(LR_uc, 1)\n",
    "\n",
    "    # IND\n",
    "    I0, I1 = I[:-1], I[1:]\n",
    "    N00 = np.sum((I0==0)&(I1==0))\n",
    "    N01 = np.sum((I0==0)&(I1==1))\n",
    "    N10 = np.sum((I0==1)&(I1==0))\n",
    "    N11 = np.sum((I0==1)&(I1==1))\n",
    "   \n",
    "    pi01 = np.clip(N01 / max(N00+N01,1), 1e-12, 1-1e-12)\n",
    "    pi11 = np.clip(N11 / max(N10+N11,1), 1e-12, 1-1e-12)\n",
    "    def L_markov(N00,N01,N10,N11,pi01,pi11):\n",
    "        s = 0.0\n",
    "        for Nxy, p in [(N01,pi01),(N00,1-pi01),(N11,pi11),(N10,1-pi11)]:\n",
    "            if p in (0,1): return -np.inf\n",
    "            s += Nxy*np.log(p)\n",
    "        return s\n",
    "    L1 = L_markov(N00,N01,N10,N11,pi01,pi11)\n",
    "    T = N00+N01+N10+N11\n",
    "    pbar = np.clip((N01+N11) / max(N00+N01+N10+N11,1), 1e-12, 1-1e-12)\n",
    "    L0 = L_markov(N00,N01,N10,N11,pbar,pbar)\n",
    "    LR_ind = -2*(L0 - L1); p_ind = 1 - chi2.cdf(LR_ind, 1)\n",
    "\n",
    "    # CC\n",
    "    LR_cc = LR_uc + LR_ind; p_cc = 1 - chi2.cdf(LR_cc, 2)\n",
    "\n",
    "    return {\"n\": int(n), \"violations\": int(n1), \"hit_rate\": p_hat,\n",
    "            \"LR_uc\": float(LR_uc), \"p_uc\": float(p_uc),\n",
    "            \"LR_ind\": float(LR_ind), \"p_ind\": float(p_ind),\n",
    "            \"LR_cc\": float(LR_cc), \"p_cc\": float(p_cc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = to_returns_monthly(df_monthly[\"PPI\"]).values  \n",
    "fr_g = fit_garch_mle(y1, dist=\"gaussian\")\n",
    "fr_t = fit_garch_mle(y1, dist=\"t\")\n",
    "fr_b = fit_garch_mle(y1, dist=\"beta\")\n",
    "alpha = 0.01\n",
    "var1_t = one_step_var_series(y1, fr_t, alpha)\n",
    "bt1_t  = var_backtests(y1, var1_t, alpha)\n",
    "h = 5\n",
    "varh_t = h_step_var_series(y1, fr_t, alpha, h, method=\"mc\", mc_paths=30000)\n",
    "\n",
    "def rolling_sum(y: np.ndarray, h: int) -> np.ndarray:\n",
    "    y = np.asarray(y, float)\n",
    "    n = y.size\n",
    "    if h == 1:\n",
    "        return y.copy()\n",
    "    out = np.full(n, np.nan)\n",
    "    conv = np.convolve(y, np.ones(h), mode=\"valid\")\n",
    "    out[h-1:] = conv\n",
    "    return out\n",
    "\n",
    "Rh   = rolling_sum(y1, h)\n",
    "bt_h = var_backtests(Rh, varh_t, alpha)\n",
    "\n",
    "results = {\n",
    "    \"Gaussian\": fr_g.ic,\n",
    "    \"t\": fr_t.ic,\n",
    "    \"Beta\": fr_b.ic\n",
    "}\n",
    "df_ic = pd.DataFrame(results).T   \n",
    "print(df_ic)\n",
    "\n",
    "\n",
    "df_bt = pd.DataFrame({\n",
    "    \"t 1-step\": bt1_t,\n",
    "    \"t h-step\": bt_h\n",
    "}).T\n",
    "print(df_bt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "h = 3 \n",
    "y1 = to_returns_monthly(df_monthly[\"CPI\"]).values  \n",
    "var11 = {\n",
    "    \"Gaussian\": one_step_var_series(y1, fr_g, alpha),\n",
    "    \"t\":        one_step_var_series(y1, fr_t, alpha),\n",
    "    \"Beta\":     one_step_var_series(y1, fr_b, alpha),\n",
    "}\n",
    "bt11 = {name: var_backtests(y1, v, alpha) for name, v in var11.items()}\n",
    "\n",
    "Rh1 = rolling_sum(y1, h)\n",
    "varh1 = {\n",
    "    \"Gaussian\": h_step_var_series(y1, fr_g1, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "    \"t\":        h_step_var_series(y1, fr_t1, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "    \"Beta\":     h_step_var_series(y1, fr_b1, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "}\n",
    "bth1 = {name: var_backtests(Rh, v, alpha) for name, v in varh1.items()}\n",
    "df_bt11 = pd.DataFrame(bt11).T.loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "df_bth1 = pd.DataFrame(bth1).T.loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "\n",
    "df_all1 = pd.concat(\n",
    "    {\n",
    "        \"1-step\": df_bt11,\n",
    "        f\"{h}-step\": df_bth1\n",
    "    },\n",
    "    axis=0\n",
    ")\n",
    "df_all1 = df_all1.round(4)\n",
    "\n",
    "from IPython.display import display\n",
    "display(df_all1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var12 = {\n",
    "    \"Gaussian\": one_step_var_series(y2, fr_g, alpha),\n",
    "    \"t\":        one_step_var_series(y2, fr_t, alpha),\n",
    "    \"Beta\":     one_step_var_series(y2, fr_b, alpha),\n",
    "}\n",
    "bt12 = {name: var_backtests(y2, v, alpha) for name, v in var12.items()}\n",
    "\n",
    "Rh2 = rolling_sum(y2, h)\n",
    "varh2 = {\n",
    "    \"Gaussian\": h_step_var_series(y2, fr_g2, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "    \"t\":        h_step_var_series(y2, fr_t2, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "    \"Beta\":     h_step_var_series(y2, fr_b2, alpha, h, method=\"mc\", mc_paths=30000),\n",
    "}\n",
    "bth2 = {name: var_backtests(Rh, v, alpha) for name, v in varh2.items()}\n",
    "\n",
    "df_bt12 = pd.DataFrame(bt12).T.loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "df_bth2 = pd.DataFrame(bth2).T.loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "\n",
    "df_all2 = pd.concat(\n",
    "    {\n",
    "        \"1-step\": df_bt12,\n",
    "        f\"{h}-step\": df_bth2\n",
    "    },\n",
    "    axis=0\n",
    ")\n",
    "df_all2 = df_all2.round(4)\n",
    "\n",
    "display(df_all2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpi_growth = to_monthly_growth(uk_rpi)\n",
    "\n",
    "fr_g8 = fit_garch_mle(rpi_growth.values, dist=\"gaussian\")\n",
    "fr_t8 = fit_garch_mle(rpi_growth.values, dist=\"t\")\n",
    "fr_b8 = fit_garch_mle(rpi_growth.values, dist=\"beta\")\n",
    "\n",
    "alpha = 0.01\n",
    "var1_t = one_step_var_series(rpi_growth.values, fr_t, alpha)\n",
    "bt1_t  = var_backtests(rpi_growth.values, var1_t, alpha)\n",
    "print(bt1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ic_table(*fit_results):\n",
    "    rows = []\n",
    "    for fr in fit_results:\n",
    "        rows.append({\n",
    "            \"dist\": fr.dist.title(),\n",
    "            \"AIC\": fr.ic[\"AIC\"], \"BIC\": fr.ic[\"BIC\"],\n",
    "            \"AICc\": fr.ic[\"AICc\"], \"HQIC\": fr.ic[\"HQIC\"]\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"dist\").round(4)\n",
    "\n",
    "df_ic8 = ic_table(fr_g8, fr_t8, fr_b8)\n",
    "print(df_ic8)\n",
    "def backtest_tables(y, fr_g, fr_t, fr_b, alpha=0.01, h=3, method=\"mc\", mc_paths=30000):\n",
    "    # 1-step\n",
    "    var1 = {\n",
    "        \"Gaussian\": one_step_var_series(y, fr_g, alpha),\n",
    "        \"t\":        one_step_var_series(y, fr_t, alpha),\n",
    "        \"Beta\":     one_step_var_series(y, fr_b, alpha),\n",
    "    }\n",
    "    bt1 = {k: var_backtests(y, v, alpha) for k, v in var1.items()}\n",
    "    df_bt1 = (pd.DataFrame(bt1).T\n",
    "                .loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "                .round(4))\n",
    "\n",
    "    # h-step\n",
    "    Rh = rolling_sum(y, h)\n",
    "    varh = {\n",
    "        \"Gaussian\": h_step_var_series(y, fr_g, alpha, h, method=method, mc_paths=mc_paths),\n",
    "        \"t\":        h_step_var_series(y, fr_t, alpha, h, method=method, mc_paths=mc_paths),\n",
    "        \"Beta\":     h_step_var_series(y, fr_b, alpha, h, method=method, mc_paths=mc_paths),\n",
    "    }\n",
    "    bth = {k: var_backtests(Rh, v, alpha) for k, v in varh.items()}\n",
    "    df_bth = (pd.DataFrame(bth).T\n",
    "                .loc[:, [\"n\",\"violations\",\"hit_rate\",\"LR_uc\",\"p_uc\",\"LR_ind\",\"p_ind\",\"LR_cc\",\"p_cc\"]]\n",
    "                .round(4))\n",
    "    return df_bt1, df_bth\n",
    "\n",
    "df_bt18, df_bth8 = backtest_tables(rpi_growth.values, fr_g8, fr_t8, fr_b8, alpha=0.05, h=3)\n",
    "#print(\"1-step VaR backtests:\\n\", df_bt1, \"\\n\")\n",
    "#print(\"3-step VaR backtests:\\n\", df_bth)\n",
    "from IPython.display import display\n",
    "#display(df_ic)\n",
    "display(df_bt18)\n",
    "display(df_bth8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, df[col].first_valid_index(), \"→\", df[col].last_valid_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "\n",
    "start, end = \"1900-01-01\", \"2025-12-31\"\n",
    "\n",
    "FRED_IDS = {\n",
    "    \"Euro_HICP\": \"CP0000EZ19M086NEST\",     \n",
    "    \"Japan_CPI\": \"JPNCPIALLMINMEI\",        \n",
    "    \"China_CPI\": \"CHNCPIALLMINMEI\",      \n",
    "}\n",
    "\n",
    "df_PI = pd.DataFrame()\n",
    "for name, fid in FRED_IDS.items():\n",
    "    try:\n",
    "        s = web.DataReader(fid, \"fred\", start, end)\n",
    "        s.name = name\n",
    "        df_PI[name] = s\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed: {name} ({fid}): {e}\")\n",
    "\n",
    "#print(df_PI.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_PI.columns:\n",
    "    print(col, df_PI[col].first_valid_index(), \"→\", df_PI[col].last_valid_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_monthly_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.resample(\"M\").last().asfreq(\"M\")\n",
    "\n",
    "def to_growth(series: pd.Series) -> pd.Series:\n",
    "    s = series.dropna()\n",
    "    if (s <= 0).any():\n",
    "        return s.diff().dropna()\n",
    "    else:\n",
    "        return np.log(s).diff().dropna()\n",
    "\n",
    "df_monthly = to_monthly_index(df_PI)\n",
    "rets = df_monthly.apply(to_growth).dropna(how=\"all\")\n",
    "\n",
    "print(\"usable columns:\", list(rets.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moments = pd.DataFrame({\n",
    "    \"n\": rets.count(),\n",
    "    \"skew\": rets.skew(),\n",
    "    \"kurt_excess\": rets.kurtosis(),\n",
    "})\n",
    "print(moments.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Euro_HICP\" \n",
    "if col not in rets.columns:\n",
    "    raise ValueError(f\"{col} not in the data, can use column:{list(rets.columns)}\")\n",
    "\n",
    "y = rets[col].dropna().values\n",
    "\n",
    "fr_g = fit_garch_mle(y, dist=\"gaussian\")\n",
    "fr_t = fit_garch_mle(y, dist=\"t\")\n",
    "fr_b = fit_garch_mle(y, dist=\"beta\")\n",
    "\n",
    "df_ic = pd.DataFrame({\n",
    "    \"Gaussian\": fr_g.ic,\n",
    "    \"t\": fr_t.ic,\n",
    "    \"Beta\": fr_b.ic\n",
    "}).T\n",
    "print(df_ic.round(4))\n",
    "\n",
    "# VaR test（1-step / h-step）\n",
    "from IPython.display import display\n",
    "\n",
    "alpha = 0.05; h = 10\n",
    "df_bt1, df_bth = backtest_tables(y, fr_g, fr_t, fr_b, alpha=alpha, h=h, method=\"mc\", mc_paths=30000)\n",
    "display(df_bt1)   # 1-step\n",
    "display(df_bth)   # h-step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col4 = \"Euro_HICP\"  \n",
    "if col4 not in rets.columns:\n",
    "    raise ValueError(f\"{col4} not in the data, can use column:{list(rets.columns)}\")\n",
    "\n",
    "y4 = rets[col4].dropna().values\n",
    "\n",
    "fr_g4 = fit_garch_mle(y4, dist=\"gaussian\")\n",
    "fr_t4 = fit_garch_mle(y4, dist=\"t\")\n",
    "fr_b4 = fit_garch_mle(y4, dist=\"beta\")\n",
    "\n",
    "df_ic4 = pd.DataFrame({\n",
    "    \"Gaussian\": fr_g4.ic,\n",
    "    \"t\": fr_t4.ic,\n",
    "    \"Beta\": fr_b4.ic\n",
    "}).T\n",
    "print(df_ic4.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05; h = 3\n",
    "df_bt14, df_bth4 = backtest_tables(y4, fr_g4, fr_t4, fr_b4, alpha=alpha, h=h, method=\"mc\", mc_paths=30000)\n",
    "display(df_bt1)   # 1-step\n",
    "display(df_bth4)   # h-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col5 = \"China_CPI\"  \n",
    "if col5 not in rets.columns:\n",
    "    raise ValueError(f\"{col5} not in the data, can use column:{list(rets.columns)}\")\n",
    "\n",
    "y5 = rets[col5].dropna().values\n",
    "\n",
    "fr_g5 = fit_garch_mle(y5, dist=\"gaussian\")\n",
    "fr_t5 = fit_garch_mle(y5, dist=\"t\")\n",
    "fr_b5 = fit_garch_mle(y5, dist=\"beta\")\n",
    "\n",
    "df_ic5 = pd.DataFrame({\n",
    "    \"Gaussian\": fr_g5.ic,\n",
    "    \"t\": fr_t5.ic,\n",
    "    \"Beta\": fr_b5.ic\n",
    "}).T\n",
    "print(df_ic5.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05; h = 3\n",
    "df_bt15, df_bth5 = backtest_tables(y5, fr_g5, fr_t5, fr_b5, alpha=alpha, h=h, method=\"mc\", mc_paths=30000)\n",
    "display(df_bt15)   # 1-step\n",
    "display(df_bth5)   # h-step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col6 = \"Japan_CPI\"  \n",
    "if col6 not in rets.columns:\n",
    "    raise ValueError(f\"{col6} not in the data, can use column:{list(rets.columns)}\")\n",
    "\n",
    "y6 = rets[col6].dropna().values\n",
    "\n",
    "fr_g6 = fit_garch_mle(y6, dist=\"gaussian\")\n",
    "fr_t6 = fit_garch_mle(y6, dist=\"t\")\n",
    "fr_b6 = fit_garch_mle(y6, dist=\"beta\")\n",
    "\n",
    "df_ic6 = pd.DataFrame({\n",
    "    \"Gaussian\": fr_g6.ic,\n",
    "    \"t\": fr_t6.ic,\n",
    "    \"Beta\": fr_b6.ic\n",
    "}).T\n",
    "print(df_ic6.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05; h = 3\n",
    "df_bt16, df_bth6 = backtest_tables(y6, fr_g6, fr_t6, fr_b6, alpha=alpha, h=h, method=\"mc\", mc_paths=30000)\n",
    "display(df_bt16)   # 1-step\n",
    "display(df_bth6)   # h-step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Put your series here ---\n",
    "series_dict = {\n",
    "    \"US CPI\": returns[\"CPI\"].dropna(),\n",
    "    \"UK RPI\": rpi_growth.dropna(),\n",
    "    \"Euro HICP\": rets[\"Euro_HICP\"].dropna(),\n",
    "    \"Japan CPI\": rets[\"Japan_CPI\"].dropna(),\n",
    "    \"China CPI\": rets[\"China_CPI\"].dropna(),\n",
    "    \"US PPI\": returns[\"PPI\"].dropna()\n",
    "}\n",
    "\n",
    "# convert to % growth\n",
    "series_dict = {k: v * 100 for k, v in series_dict.items()}\n",
    "\n",
    "# global axis limits\n",
    "all_data = pd.concat(series_dict.values())\n",
    "x_lo, x_hi = np.quantile(all_data, [0.005, 0.995])   # same x range\n",
    "y_max = 0\n",
    "\n",
    "# precompute global y max for density\n",
    "for s in series_dict.values():\n",
    "    mu, sd = s.mean(), s.std(ddof=0)\n",
    "    x = np.linspace(x_lo, x_hi, 400)\n",
    "    hist_vals, _ = np.histogram(s[(s >= x_lo) & (s <= x_hi)], bins=30, density=True)\n",
    "    y_max = max(y_max, hist_vals.max(), norm.pdf(x, mu, sd).max())\n",
    "\n",
    "# --- Plot each series separately but consistent scales ---\n",
    "for name, s in series_dict.items():\n",
    "    mu, sd = s.mean(), s.std(ddof=0)\n",
    "    x = np.linspace(x_lo, x_hi, 400)\n",
    "\n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.hist(s[(s >= x_lo) & (s <= x_hi)], bins=30, density=True,\n",
    "             edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "    if sd > 0:\n",
    "        plt.plot(x, norm.pdf(x, mu, sd), \"r\", lw=2, label=\"Fitted Normal\")\n",
    "\n",
    "    #plt.title(f\"Skewness={s.skew():.3f}, Excess Kurtosis={s.kurtosis():.3f}\", fontsize=20)\n",
    "    plt.xlabel(\"Monthly log-difference\", fontsize=30)\n",
    "    plt.ylabel(\"Density\", fontsize=30)\n",
    "    plt.xlim(x_lo, x_hi)\n",
    "    plt.ylim(0, y_max * 1.05)   # same y scale for all\n",
    "    plt.legend(loc=\"upper right\", fontsize=28) \n",
    "    plt.tick_params(axis='x', labelsize=30)  \n",
    "    plt.tick_params(axis='y', labelsize=30)  \n",
    "    plt.savefig(f\"{name.replace(' ', '_')}_Thist.pdf\", dpi=300)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
